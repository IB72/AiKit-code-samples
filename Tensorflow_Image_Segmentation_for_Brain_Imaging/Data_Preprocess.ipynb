{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***There are 2 parts to this example. You can either use the SMALL BraTs dataset included in this tutorial or download the LARGE BraTs dataset to preprocess and train.*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset (optional step for large dataset) \n",
    "\n",
    "As mentioned earlier, the small dataset is included in this tutorial for users to run a quick test on this demo. However, the large BraTs dataset must be downloaded from the Medical Segmentation DecathlonÂ website. \n",
    "BraTs dataset is hosted on a public Google drive which requires gdown python package. \n",
    "Note that the size of the dataset is about 7GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install gdown\n",
    "# gdown https://drive.google.com/uc?id=1A2IU8Sgea1h3fYLpYtFb2v7NYdMjvEhU\n",
    "# tar -xf Task01_BrainTumour.tar --directory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess BraTs (small and large) Dataset\n",
    "\n",
    " This step converts the Medical Decathlon raw Nifti files into a single HDF5 file. We will also convert the data dimension for both features and labels. Each image is 4D with array shape (240, 240, 155,4). The first 2 indexes contain data size on X and Y planes (voxels), 3rd index on MRI scan slices, 4rd index on 4 types of modality or types of scans. The metadata \"dataset.json\" is available at the downloaded location for the large dataset\n",
    " \n",
    "The goal of this step is to convert the 4D training Nifti images and labels(masks) into a stack of 2D slices stored as a HDF5 file. The preprocessing steps include cropping the center to 144x144 and normalizing the image amongst others. For this experiment, we will not be using the validation/test set downloaded as part of the dataset, but rather a portion of the training set. \n",
    "The training set in the large dataset contains 484 unique MRI scans on which we perform 85:15 split and train on 80% data and do a 50:50 split on the remaining 20% data for validation and testing phases. \n",
    " \n",
    "Each image has the following size before and after processing \n",
    "\n",
    "|            Raw data            |         Processed data         |\n",
    "|:------------------------------:|:------------------------------:|\n",
    "| image shape=(240, 240, 155, 4) | image shape=(144, 144, 144, 4) |\n",
    "| mask shape=(240, 240, 155)     | mask shape=(144,144,144,1)     |\n",
    "\n",
    "Post-processing, the datasize of train, validate and test images will be\n",
    "\n",
    "Training image dimensions:   (58464, 144, 144, 4)<br>\n",
    "Training mask dimensions:    (58464, 144, 144, 1)<br>\n",
    "Validation image dimensions: (4608, 144, 144, 4)<br>\n",
    "Validation mask dimensions:  (4608, 144, 144, 1)<br>\n",
    "Testing image dimensions: (6624, 144, 144, 4)<br>\n",
    "Testing mask dimensions:  (6624, 144, 144, 1)<br>\n",
    "\n",
    "Below is the structure post processing. The size of the small dataset attached in the dataset for testing purpose has 3, 1, 1 train, validation, test images, respectively.\n",
    "\n",
    "Training image dimensions:   (432, 144, 144, 4)<br>\n",
    "Training mask dimensions:    (432, 144, 144, 1)<br>\n",
    "Validation image dimensions: (144, 144, 144, 4)<br>\n",
    "Validation mask dimensions:  (144, 144, 144, 1)<br>\n",
    "Testing image dimensions: (144, 144, 144, 4)<br>\n",
    "Testing mask dimensions:  (144, 144, 144, 1)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code uses \"small_data\" path. **If you have downloaded BraTs dataset, uncomment the parser command below to add the large BraTs dataset path to `--data-path` and comment the former**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed so that always get same random mix\n",
    "\n",
    "import os\n",
    "import nibabel as nib  # pip install nibabel\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # pip install tqdm\n",
    "import h5py   # pip install h5py\n",
    "import json\n",
    "\n",
    "import sys; sys.argv=['']; del sys #Make argsparser work in Jupyter notebooks\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Convert Decathlon raw Nifti data (http://medicaldecathlon.com) files to Numpy data files\",\n",
    "    add_help=True, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "parser.add_argument(\"--data_path\",default=\"data/small_data/\",help=\"Path to the raw BraTS datafiles\")\n",
    "#parser.add_argument(\"--data_path\",default=\"data/Task01_BrainTumour/\",help=\"Path to the raw BraTS datafiles\")\n",
    "parser.add_argument(\"--save_path\",default=\"data\",help=\"Folder to save Numpy data files\")\n",
    "parser.add_argument(\"--output_filename\",default=\"Task01_BrainTumour.h5\",help=\"Name of the output HDF5 file\")\n",
    "parser.add_argument(\"--resize\", type=int, default=144, help=\"Resize height and width to this size. Original size = 240\")\n",
    "parser.add_argument(\"--split\", type=float, default=0.85, help=\"Train/test split ratio\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and display the metadata information of the `dataset.json` on image and label file names. \n",
    "Load the `dataset.json` to create a json object `experiment_data`. This variable will be used through out the tutorial to access images and labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = os.path.join(args.data_path, \"dataset.json\")\n",
    "\n",
    "try:\n",
    "    with open(json_filename, \"r\") as fp:\n",
    "        experiment_data = json.load(fp)\n",
    "except IOError as e:\n",
    "    print(\"File {} doesn't exist. It should be part of the \"\" Decathlon directory\".format(json_filename))\n",
    "\n",
    "# Print information about the Decathlon experiment data\n",
    "print(\"*\" * 30)\n",
    "print(\"=\" * 30)\n",
    "print(\"Dataset name:        \", experiment_data[\"name\"])\n",
    "print(\"Dataset description: \", experiment_data[\"description\"])\n",
    "print(\"Tensor image size:   \", experiment_data[\"tensorImageSize\"])\n",
    "print(\"Dataset release:     \", experiment_data[\"release\"])\n",
    "print(\"Dataset reference:   \", experiment_data[\"reference\"])\n",
    "print(\"Dataset license:     \", experiment_data[\"licence\"])  # sic\n",
    "print(\"=\" * 30)\n",
    "print(\"*\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are croping and normalizaing functions that will be used at the time of dataset preprocessing and conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center(img, cropx, cropy, cropz):\n",
    "    \"\"\"\n",
    "    Take a center crop of the images.\n",
    "    If we are using a 2D model, then we'll just stack the\n",
    "    z dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y, z, c = img.shape\n",
    "\n",
    "    # Make sure starting index is >= 0\n",
    "    startx = max(x // 2 - (cropx // 2), 0)\n",
    "    starty = max(y // 2 - (cropy // 2), 0)\n",
    "    startz = max(z // 2 - (cropz // 2), 0)\n",
    "\n",
    "    # Make sure ending index is <= size\n",
    "    endx = min(startx + cropx, x)\n",
    "    endy = min(starty + cropy, y)\n",
    "    endz = min(startz + cropz, z)\n",
    "\n",
    "    return img[startx:endx, starty:endy, startz:endz, :]\n",
    "\n",
    "\n",
    "def normalize_img(img):\n",
    "    \"\"\"\n",
    "    Normalize the pixel values.\n",
    "    This is one of the most important preprocessing steps.\n",
    "    We need to make sure that the pixel values have a mean of 0\n",
    "    and a standard deviation of 1 to help the model to train\n",
    "    faster and more accurately.\n",
    "    \"\"\"\n",
    "\n",
    "    for channel in range(img.shape[3]):\n",
    "        img[:, :, :, channel] = (\n",
    "            img[:, :, :, channel] - np.mean(img[:, :, :, channel])) \\\n",
    "            / np.std(img[:, :, :, channel])\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def attach_attributes(df, json_data, name):\n",
    "    \"\"\"\n",
    "    Save the json data\n",
    "    \"\"\"\n",
    "\n",
    "    if type(json_data) is str:\n",
    "        length = 1\n",
    "    else:\n",
    "        length = len(json_data)\n",
    "\n",
    "    dt = h5py.special_dtype(vlen=str)\n",
    "    dset = df.create_dataset(name, (length,), dtype=dt)\n",
    "    dset[:] = json_data\n",
    "\n",
    "\n",
    "def preprocess_inputs(img):\n",
    "    \"\"\"\n",
    "    Process the input images\n",
    "\n",
    "    For BraTS subset:\n",
    "    INPUT CHANNELS:  \"modality\": {\n",
    "         \"0\": \"FLAIR\", T2-weighted-Fluid-Attenuated Inversion Recovery MRI\n",
    "         \"1\": \"T1w\",  T1-weighted MRI\n",
    "         \"2\": \"t1gd\", T1-gadolinium contrast MRI\n",
    "         \"3\": \"T2w\"   T2-weighted MRI\n",
    "     }\n",
    "    \"\"\"\n",
    "    if len(img.shape) != 4:  # Make sure 4D\n",
    "        img = np.expand_dims(img, -1)\n",
    "\n",
    "    img = crop_center(img, args.resize, args.resize, args.resize)\n",
    "    img = normalize_img(img)\n",
    "\n",
    "    img = np.swapaxes(np.array(img), 0, -2)\n",
    "\n",
    "    # img = img[:,:,:,[0]]  # Just get the FLAIR channel\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_labels(msk):\n",
    "    \"\"\"\n",
    "    Process the ground truth labels\n",
    "\n",
    "    For BraTS subset:\n",
    "    LABEL_CHANNELS: \"labels\": {\n",
    "         \"0\": \"background\",  No tumor\n",
    "         \"1\": \"edema\",       Swelling around tumor\n",
    "         \"2\": \"non-enhancing tumor\",  Tumor that isn't enhanced by Gadolinium contrast\n",
    "         \"3\": \"enhancing tumour\"  Gadolinium contrast enhanced regions\n",
    "     }\n",
    "\n",
    "    \"\"\"\n",
    "    if len(msk.shape) != 4:  # Make sure 4D\n",
    "        msk = np.expand_dims(msk, -1)\n",
    "\n",
    "    msk = crop_center(msk, args.resize, args.resize, args.resize)\n",
    "\n",
    "    # Combining all masks assumes that a mask value of 0 is the background\n",
    "    msk[msk > 1] = 1  # Combine all masks\n",
    "    msk = np.swapaxes(np.array(msk), 0, -2)\n",
    "\n",
    "    return msk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below runs the `convert_raw_data_to_hdf5` function that goes through the Decathlon `dataset.json` file.<br>\n",
    "The code will:<br>\n",
    "    a. Read in Nifti (`*.nii`) format files, and the calls necessary crop and normalize functions<br>\n",
    "    b. Convert the 3D images and masks into a stack of 2D slices<br>\n",
    "    c. Finally, saves the result in an HDF5 format<br>\n",
    "\n",
    "This procedure is implemented on training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_data_to_hdf5(trainIdx, validateIdx, testIdx, fileIdx,filename, dataDir, json_data):\n",
    "\n",
    "    hdf_file = h5py.File(filename, \"w\")\n",
    "\n",
    "    # Save the dataset attributes\n",
    "    attach_attributes(hdf_file, str(json_data[\"modality\"]), \"modalities\")\n",
    "    attach_attributes(hdf_file, json_data[\"licence\"], \"license\")\n",
    "    attach_attributes(hdf_file, json_data[\"reference\"], \"reference\")\n",
    "    attach_attributes(hdf_file, json_data[\"name\"], \"name\")\n",
    "    attach_attributes(hdf_file, json_data[\"description\"], \"description\")\n",
    "    attach_attributes(hdf_file, json_data[\"release\"], \"release\")\n",
    "    attach_attributes(\n",
    "        hdf_file, json_data[\"tensorImageSize\"], \"tensorImageSize\")\n",
    "\n",
    "    # Training filenames\n",
    "    train_image_files = []\n",
    "    train_label_files = []\n",
    "    print(train_label_files)\n",
    "    for idx in trainIdx:\n",
    "        train_image_files.append(fileIdx[idx][\"image\"])\n",
    "        train_label_files.append(fileIdx[idx][\"label\"])\n",
    "\n",
    "    # Validation filenames\n",
    "    validate_image_files = []\n",
    "    validate_label_files = []\n",
    "    for idx in validateIdx:\n",
    "        validate_image_files.append(fileIdx[idx][\"image\"])\n",
    "        validate_label_files.append(fileIdx[idx][\"label\"])\n",
    "\n",
    "    # Testing filenames\n",
    "    test_image_files = []\n",
    "    test_label_files = []\n",
    "    for idx in testIdx:\n",
    "        test_image_files.append(fileIdx[idx][\"image\"])\n",
    "        test_label_files.append(fileIdx[idx][\"label\"])\n",
    "\n",
    "    attach_attributes(hdf_file, train_image_files, \"training_input_files\")\n",
    "    attach_attributes(hdf_file, train_label_files, \"training_label_files\")\n",
    "    attach_attributes(hdf_file, validate_image_files, \"validation_input_files\")\n",
    "    attach_attributes(hdf_file, validate_label_files, \"validation_label_files\")\n",
    "    attach_attributes(hdf_file, test_image_files, \"testing_input_files\")\n",
    "    attach_attributes(hdf_file, test_label_files, \"testing_label_files\")\n",
    "\n",
    "    \"\"\"\n",
    "    Print shapes of raw data\n",
    "    \"\"\"\n",
    "    print(\"Data shapes\")\n",
    "    print(\"===========\")\n",
    "    print(\"n.b. All tensors converted to stacks of 2D slices.\")\n",
    "    print(\"If you want true 3D tensors, then modify this code appropriately.\")\n",
    "    data_filename = os.path.join(dataDir, train_image_files[0])\n",
    "    img = np.array(nib.load(data_filename).dataobj)\n",
    "    print(\"Raw Image shape     = \", img.shape)\n",
    "    crop_shape = preprocess_inputs(img).shape[1:]\n",
    "    print(\"Cropped Image shape = (?, {}, {}, {})\".format(crop_shape[0],\n",
    "                                                         crop_shape[1],\n",
    "                                                         crop_shape[2]))\n",
    "\n",
    "    data_filename = os.path.join(dataDir, train_label_files[0])\n",
    "    msk = np.array(nib.load(data_filename).dataobj)\n",
    "    print(\"Raw Masks shape     = \", msk.shape)\n",
    "    crop_shape = preprocess_labels(msk).shape[1:]\n",
    "    print(\"Cropped Masks shape = (?, {}, {}, {})\".format(crop_shape[0],\n",
    "                                                         crop_shape[1],\n",
    "                                                         crop_shape[2]))\n",
    "\n",
    "    # Save training set images\n",
    "    print(\"Step 1 of 6. Save training set images.\")\n",
    "    first = True\n",
    "    for idx in tqdm(train_image_files):\n",
    "\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        img = np.array(nib.load(data_filename).dataobj)\n",
    "\n",
    "        img = preprocess_inputs(img)\n",
    "        num_rows = img.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            img_train_dset = hdf_file.create_dataset(\"imgs_train\",\n",
    "                                                     img.shape,\n",
    "                                                     maxshape=(None,\n",
    "                                                               img.shape[1],\n",
    "                                                               img.shape[2],\n",
    "                                                               img.shape[3]),\n",
    "                                                     dtype=float,\n",
    "                                                     compression=\"gzip\")\n",
    "            img_train_dset[:] = img\n",
    "        else:\n",
    "            row = img_train_dset.shape[0]  # Count current dataset rows\n",
    "            img_train_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            img_train_dset[row:(row + num_rows), :] = img\n",
    "\n",
    "    # Save validation set images\n",
    "    print(\"Step 2 of 6. Save validation set images.\")\n",
    "    first = True\n",
    "    for idx in tqdm(validate_image_files):\n",
    "\n",
    "        # Nibabel should read the file as X,Y,Z,C\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        img = np.array(nib.load(data_filename).dataobj)\n",
    "        img = preprocess_inputs(img)\n",
    "\n",
    "        num_rows = img.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            img_validation_dset = hdf_file.create_dataset(\"imgs_validation\",\n",
    "                                                          img.shape,\n",
    "                                                          maxshape=(None,\n",
    "                                                                    img.shape[1],\n",
    "                                                                    img.shape[2],\n",
    "                                                                    img.shape[3]),\n",
    "                                                          dtype=float,\n",
    "                                                          compression=\"gzip\")\n",
    "            img_validation_dset[:] = img\n",
    "        else:\n",
    "            row = img_validation_dset.shape[0]  # Count current dataset rows\n",
    "            img_validation_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            img_validation_dset[row:(row + num_rows), :] = img\n",
    "\n",
    "    # Save validation set images\n",
    "    print(\"Step 3 of 6. Save testing set images.\")\n",
    "    first = True\n",
    "    for idx in tqdm(test_image_files):\n",
    "\n",
    "        # Nibabel should read the file as X,Y,Z,C\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        img = np.array(nib.load(data_filename).dataobj)\n",
    "        img = preprocess_inputs(img)\n",
    "\n",
    "        num_rows = img.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            img_testing_dset = hdf_file.create_dataset(\"imgs_testing\",\n",
    "                                                       img.shape,\n",
    "                                                       maxshape=(None,\n",
    "                                                                 img.shape[1],\n",
    "                                                                 img.shape[2],\n",
    "                                                                 img.shape[3]),\n",
    "                                                       dtype=float,\n",
    "                                                       compression=\"gzip\")\n",
    "            img_testing_dset[:] = img\n",
    "        else:\n",
    "            row = img_testing_dset.shape[0]  # Count current dataset rows\n",
    "            img_testing_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            img_testing_dset[row:(row + num_rows), :] = img\n",
    "\n",
    "    # Save training set masks\n",
    "    print(\"Step 4 of 6. Save training set masks.\")\n",
    "    first = True\n",
    "    for idx in tqdm(train_label_files):\n",
    "\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        msk = np.array(nib.load(data_filename).dataobj)\n",
    "        msk = preprocess_labels(msk)\n",
    "        num_rows = msk.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            msk_train_dset = hdf_file.create_dataset(\"msks_train\",\n",
    "                                                     msk.shape,\n",
    "                                                     maxshape=(None,\n",
    "                                                               msk.shape[1],\n",
    "                                                               msk.shape[2],\n",
    "                                                               msk.shape[3]),\n",
    "                                                     dtype=float,\n",
    "                                                     compression=\"gzip\")\n",
    "            msk_train_dset[:] = msk\n",
    "        else:\n",
    "            row = msk_train_dset.shape[0]  # Count current dataset rows\n",
    "            msk_train_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            msk_train_dset[row:(row + num_rows), :] = msk\n",
    "\n",
    "    # Save testing/validation set masks\n",
    "\n",
    "    print(\"Step 5 of 6. Save validation set masks.\")\n",
    "    first = True\n",
    "    for idx in tqdm(validate_label_files):\n",
    "\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        msk = np.array(nib.load(data_filename).dataobj)\n",
    "        msk = preprocess_labels(msk)\n",
    "\n",
    "        num_rows = msk.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            msk_validation_dset = hdf_file.create_dataset(\"msks_validation\",\n",
    "                                                          msk.shape,\n",
    "                                                          maxshape=(None,\n",
    "                                                                    msk.shape[1],\n",
    "                                                                    msk.shape[2],\n",
    "                                                                    msk.shape[3]),\n",
    "                                                          dtype=float,\n",
    "                                                          compression=\"gzip\")\n",
    "            msk_validation_dset[:] = msk\n",
    "        else:\n",
    "            row = msk_validation_dset.shape[0]  # Count current dataset rows\n",
    "            msk_validation_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            msk_validation_dset[row:(row + num_rows), :] = msk\n",
    "\n",
    "    print(\"Step 6 of 6. Save testing set masks.\")\n",
    "    first = True\n",
    "    for idx in tqdm(test_label_files):\n",
    "\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        msk = np.array(nib.load(data_filename).dataobj)\n",
    "        msk = preprocess_labels(msk)\n",
    "\n",
    "        num_rows = msk.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            msk_testing_dset = hdf_file.create_dataset(\"msks_testing\",\n",
    "                                                       msk.shape,\n",
    "                                                       maxshape=(None,\n",
    "                                                                 msk.shape[1],\n",
    "                                                                 msk.shape[2],\n",
    "                                                                 msk.shape[3]),\n",
    "                                                       dtype=float,\n",
    "                                                       compression=\"gzip\")\n",
    "            msk_testing_dset[:] = msk\n",
    "        else:\n",
    "            row = msk_testing_dset.shape[0]  # Count current dataset rows\n",
    "            msk_testing_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            msk_testing_dset[row:(row + num_rows), :] = msk\n",
    "\n",
    "    hdf_file.close()\n",
    "    print(\"Finished processing.\")\n",
    "    print(\"HDF5 saved to {}\".format(filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the functions have been defined, let's proceed with converting the Decathlon raw Nifti data files to a single training and validation HDF5 data file. The below snippet sets the output file structure and removes any exisitng hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\n",
    "args.save_path, \"{}x{}/\".format(args.resize, args.resize))\n",
    "\n",
    "# Create directory\n",
    "try:\n",
    "    os.makedirs(save_dir)\n",
    "except OSError:\n",
    "    if not os.path.isdir(save_dir):\n",
    "        raise\n",
    "\n",
    "filename = os.path.join(save_dir, args.output_filename)\n",
    "# Check for existing output file and delete if exists\n",
    "if os.path.exists(filename):\n",
    "    print(\"Removing existing data file: {}\".format(filename))\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will use the `experiment_data` to access the image and label lists to randomize the training dataset. Since the BraTs dataset (both small and large) doesn't have labels for test dataset, we will split the training data into train/val/test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed so that always get same random mix\n",
    "np.random.seed(816)\n",
    "numFiles = experiment_data[\"numTraining\"]\n",
    "idxList = np.arange(numFiles)  # List of file indices\n",
    "randomList = np.random.random(numFiles)  # List of random numbers\n",
    "# Random number go from 0 to 1. So anything above\n",
    "# args.train_split is in the validation list.\n",
    "trainList = idxList[randomList < args.split]\n",
    "otherList = idxList[randomList >= args.split]\n",
    "randomList = np.random.random(len(otherList))  # List of random numbers\n",
    "validateList = otherList[randomList >= 0.5]\n",
    "testList = otherList[randomList < 0.5]\n",
    "convert_raw_data_to_hdf5(trainList, validateList, testList,\n",
    "                         experiment_data[\"training\"],\n",
    "                         filename, args.data_path,\n",
    "                         experiment_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the processed datafile `Task01_BrainTumour.h5` is located at data/144x144/Task01_BrainTumour.h5. To train a Unet model with this data file run the next notebook `Train_Unet.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. SPDX-License-Identifier: EPL-2.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Copyright (c) 2019 Intel Corporation`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
